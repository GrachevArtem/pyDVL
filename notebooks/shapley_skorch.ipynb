{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Computing Shapley values for a torch model\n",
    "\n",
    "This notebook illustrates how to wrap a `torch.nn.Module` model using [skorch](https://skorch.readthedocs.io/en/stable/) and use it with pyDVL to compute Shapley values. The model is a simple convolutional neural network (CNN) that classifies handwritten digits from the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.\n",
    "\n",
    "The notebook follows almost verbatim the first few steps of the notebook on MSR Banzhaf."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from support.common import filecache\n",
    "from support.datasets import load_digits_dataset\n",
    "\n",
    "from pydvl.reporting.plots import plot_result_errors\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ioff()  # Prevent jupyter from automatically plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 6)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"xtick.labelsize\"] = 10\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10\n",
    "plt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\n",
    "plt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n",
    "\n",
    "is_CI = os.environ.get(\"CI\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random_state = 36\n",
    "\n",
    "n_jobs = 6\n",
    "n_epochs = 24\n",
    "batch_size = 128\n",
    "random.seed(random_state)\n",
    "torch.manual_seed(random_state);"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The dataset\n",
    "\n",
    "The data consists of ~1800 grayscale images of 8x8 pixels with 16 shades of gray. These images contain handwritten digits from 0 to 9. The helper function `load_digits_dataset()` downloads and prepares it for usage returning two [Datasets][pydvl.valuation.dataset.Dataset]."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train, test = load_digits_dataset(\n",
    "    train_size=0.7, random_state=random_state, device=device\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(4):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(train.data().cpu().x[i].reshape((8, 8)), cmap=\"gray\")\n",
    "    ax.set_xlabel(f\"Label: {train.data().y[i]}\")\n",
    "plt.suptitle(\"Example images from the dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating the utility using a skorch model\n",
    "\n",
    "Now we can calculate the contribution of each training sample to the model performance. We use a simple CNN written in torch wrapped as a skorch model. Something to keep in mind is to pass `torch_load_kwargs={\"weights_only\": False}` to the `NeuralNetClassifier` constructor, otherwise the model will fail to pickle and parallelization won't work."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reduce computation time in CI\n",
    "if is_CI:\n",
    "    train = train[:10]\n",
    "    test = test[:10]\n",
    "    n_jobs = 1\n",
    "    n_epochs = 1\n",
    "    batch_size = 1\n",
    "    filecache = lambda x: lambda y: y  # passthrough"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from support.banzhaf import SimpleCNN\n",
    "\n",
    "model = NeuralNetClassifier(\n",
    "    SimpleCNN,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    lr=0.01,\n",
    "    max_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    train_split=None,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    device=device,\n",
    "    verbose=False,\n",
    "    torch_load_kwargs={\"weights_only\": False},\n",
    ")\n",
    "model.fit(*train.data());"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Training accuracy: {model.score(*train.data().cpu()):.3f}\")\n",
    "print(f\"Test accuracy: {model.score(*test.data().cpu()):.3f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As with every model-based valuation method, we need a scoring function to measure the performance of the model over the test set. We will use accuracy, but it can be anything, e.g. $R^2$, using strings from the [standard sklearn scoring methods](https://scikit-learn.org/stable/modules/model_evaluation.html), passed to [SkorchSupervisedScorer][pydvl.valuation.scorers.skorch.SkorchSupervisedScorer].\n",
    "\n",
    "We group our skorch model and the scoring function into an instance of [ModelUtility][pydvl.valuation.utility.ModelUtility]."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydvl.valuation.scorers import SkorchSupervisedScorer\n",
    "from pydvl.valuation.stopping import MinUpdates\n",
    "from pydvl.valuation.utility import ModelUtility\n",
    "\n",
    "accuracy_over_test_set = SkorchSupervisedScorer(\n",
    "    model, test_data=test, default=0.0, range=(0, 1)\n",
    ")\n",
    "\n",
    "utility = ModelUtility(model=model, scorer=accuracy_over_test_set)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In order to compute Shapley values, we use [TMCSShapleyValuation][pydvl.valuation.methods.shapley.TMCSShapleyValuation].\n",
    "\n",
    "We choose to stop computation using the [MinUpdates][pydvl.valuation.stopping.MinUpdates] stopping criterion, which terminates after a fixed number of value updates.\n",
    "\n",
    "We also define a relative [TruncationPolicy][pydvl.valuation.samplers.truncation.TruncationPolicy], which is a policy used to early-stop computation of marginal values in permutations, once the utility is close to the total utility. This is a heuristic to speed up computation introduced in the Data-Shapley paper called Truncated Monte Carlo Shapley. Note how we tell it to wait until at least 30% of every permutation has been processed in order to start evaluation. This is to ensure that noise doesn't stop the computation too early."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydvl.valuation.samplers import RelativeTruncation\n",
    "\n",
    "truncation = RelativeTruncation(rtol=0.05, burn_in_fraction=0.3)\n",
    "stopping = MinUpdates(100)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if is_CI:\n",
    "    from pydvl.valuation.stopping import MaxChecks\n",
    "\n",
    "    stopping = MaxChecks(1)  # Stop after 1 utility evaluation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now instantiate and fit the valuation. Note how parallelization is just a matter of using joblib's context manager `parallel_config` in order to set the number of jobs."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from joblib import parallel_config\n",
    "\n",
    "from pydvl.valuation.methods import TMCShapleyValuation\n",
    "\n",
    "valuation = TMCShapleyValuation(utility, truncation=truncation, is_done=stopping, progress=True)\n",
    "\n",
    "# filecache is a very simple wrapper not intended for production code\n",
    "cached_fit = filecache(\"shapley_skorch_result.pkl\")(lambda d: valuation.fit(d).result)\n",
    "with parallel_config(n_jobs=n_jobs):\n",
    "    result = cached_fit(train)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results object is of type [ValuationResult][pydvl.valuation.result.ValuationResult], and contains values, variances and number of updates of the Monte Carlo estimates. It can be indexed, sliced and copied in natural ways, as we illustrate below."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us plot the results. In the next cell we will take the 10 images with the lowest score and plot their values with 95% Normal confidence intervals."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bottom = result.sort()[:10]\n",
    "plot_result_errors(\n",
    "    bottom,\n",
    "    level=0.05,\n",
    "    title=\"Images with low values\",\n",
    "    xlabel=\"Image\",\n",
    "    ylabel=\"Banzhaf value\",\n",
    ")\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e000971326892723e7f31ded70802f690c31c3620f59a0f99e594aaee3047ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
